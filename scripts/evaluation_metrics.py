#!/usr/bin/env python3
"""
Evaluation metrics for expressive speech fine-tuning.

This module provides functions to evaluate the quality and expressivity of
speech generated by fine-tuned models. It includes metrics for:
- Pronunciation accuracy
- Prosody evaluation
- Emotion recognition/classification
- MOS (Mean Opinion Score) prediction

Usage:
    from evaluation_metrics import evaluate_expressivity, evaluate_pronunciation
    
    # Evaluate a generated sample
    expressivity_score = evaluate_expressivity(audio_path, expected_emotion)
    pronunciation_score = evaluate_pronunciation(audio_path, transcript)
"""

import os
import json
import numpy as np
import torch
import librosa
import soundfile as sf
from typing import Dict, List, Tuple, Optional, Union
from dataclasses import dataclass

# Try to import optional dependencies
try:
    import speechmetrics
    SPEECHMETRICS_AVAILABLE = True
except ImportError:
    SPEECHMETRICS_AVAILABLE = False

try:
    from resemblyzer import VoiceEncoder
    RESEMBLYZER_AVAILABLE = True
except ImportError:
    RESEMBLYZER_AVAILABLE = False

try:
    from transformers import pipeline
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False

@dataclass
class ExpressivityMetrics:
    """Class to store expressivity evaluation metrics."""
    emotion_match_score: float  # 0.0-1.0, higher is better
    emotion_confidence: float   # 0.0-1.0, higher is better
    prosody_score: float        # 0.0-1.0, higher is better
    overall_expressivity: float # 0.0-1.0, higher is better

@dataclass
class PronunciationMetrics:
    """Class to store pronunciation evaluation metrics."""
    clarity_score: float        # 0.0-1.0, higher is better
    word_error_rate: float      # 0.0-1.0, lower is better
    speaking_rate: float        # Words per second
    overall_pronunciation: float # 0.0-1.0, higher is better

@dataclass
class OverallMetrics:
    """Class to store overall evaluation metrics."""
    expressivity: ExpressivityMetrics
    pronunciation: PronunciationMetrics
    mos_prediction: float       # 1.0-5.0, higher is better

def load_audio(audio_path: str, target_sr: int = 16000) -> Tuple[np.ndarray, int]:
    """Load audio file and resample if necessary."""
    audio, sr = librosa.load(audio_path, sr=None, mono=True)
    if sr != target_sr:
        audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
        sr = target_sr
    return audio, sr

def extract_audio_features(audio: np.ndarray, sr: int) -> Dict[str, np.ndarray]:
    """Extract various features from audio for analysis."""
    features = {}
    
    # Extract pitch (fundamental frequency)
    f0, voiced_flag, voiced_probs = librosa.pyin(
        audio, fmin=librosa.note_to_hz('C2'), 
        fmax=librosa.note_to_hz('C7'), sr=sr
    )
    features['f0'] = f0
    features['voiced_flag'] = voiced_flag
    
    # Extract energy/volume
    features['rms'] = librosa.feature.rms(y=audio)[0]
    
    # Extract spectral features
    features['spectral_centroid'] = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
    features['spectral_bandwidth'] = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]
    
    # Extract tempo and rhythm features
    onset_env = librosa.onset.onset_strength(y=audio, sr=sr)
    features['tempo'], _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)
    
    # Extract MFCCs (useful for emotion recognition)
    features['mfcc'] = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
    
    return features

def evaluate_emotion_recognition(audio_path: str, expected_emotion: str) -> Dict[str, float]:
    """
    Evaluate how well the emotion in the audio matches the expected emotion.
    Uses a pre-trained emotion recognition model if available.
    
    Args:
        audio_path: Path to the audio file
        expected_emotion: Expected emotion label
        
    Returns:
        Dictionary with emotion recognition metrics
    """
    # If transformers is available, use a pre-trained emotion recognition model
    if TRANSFORMERS_AVAILABLE:
        try:
            classifier = pipeline("audio-classification", model="MIT/ast-finetuned-speech-commands-v2")
            emotion_result = classifier(audio_path)
            
            # Map expected emotion to model's emotion categories
            emotion_mapping = {
                "happy": ["happy", "joy", "excited"],
                "sad": ["sad", "unhappy", "depressed"],
                "angry": ["angry", "rage", "furious"],
                "neutral": ["neutral", "calm"],
                "fearful": ["fearful", "scared", "afraid"],
                "surprised": ["surprised", "astonished"],
                "disgust": ["disgust", "disgusted"]
            }
            
            # Find the best match for expected emotion in the classifier's results
            expected_categories = emotion_mapping.get(expected_emotion.lower(), [expected_emotion.lower()])
            
            # Check if any of the expected categories appear in the top 3 predictions
            match_score = 0.0
            top_confidence = 0.0
            for pred in emotion_result[:3]:
                pred_label = pred["label"].lower()
                for expected_cat in expected_categories:
                    if expected_cat in pred_label:
                        match_score = pred["score"]
                        break
                
                # Track the top confidence regardless of match
                if pred["score"] > top_confidence:
                    top_confidence = pred["score"]
            
            return {
                "emotion_match_score": match_score,
                "emotion_confidence": top_confidence,
                "predicted_emotions": [{"label": pred["label"], "score": pred["score"]} 
                                      for pred in emotion_result[:3]]
            }
        except Exception as e:
            print(f"Error using pre-trained emotion classifier: {e}")
    
    # Fallback: Use audio features to estimate emotion
    audio, sr = load_audio(audio_path)
    features = extract_audio_features(audio, sr)
    
    # Simple heuristic approach to emotion detection based on audio features
    # This is a fallback and not as accurate as a trained model
    
    # Calculate speech rate and variability
    speech_rate = len(features['voiced_flag']) / (len(audio) / sr)
    pitch_mean = np.nanmean(features['f0'][~np.isnan(features['f0'])])
    pitch_std = np.nanstd(features['f0'][~np.isnan(features['f0'])])
    intensity_mean = np.mean(features['rms'])
    intensity_std = np.std(features['rms'])
    
    # Simple rules for emotion detection
    emotion_scores = {
        "happy": 0.3 * (speech_rate / 5.0) + 0.4 * (pitch_mean / 300) + 0.3 * intensity_std,
        "sad": 0.5 * (1 - speech_rate / 5.0) + 0.3 * (1 - pitch_std / 50) + 0.2 * (1 - intensity_mean),
        "angry": 0.4 * intensity_mean + 0.3 * pitch_std / 50 + 0.3 * speech_rate / 5.0,
        "neutral": 1.0 - (0.33 * intensity_std + 0.33 * pitch_std / 50 + 0.34 * abs(speech_rate - 3.0) / 3.0),
        "fearful": 0.4 * (speech_rate / 5.0) + 0.4 * (pitch_mean / 300) + 0.2 * (1 - intensity_mean),
        "surprised": 0.5 * pitch_std / 50 + 0.3 * intensity_std + 0.2 * speech_rate / 5.0,
        "disgust": 0.4 * (1 - intensity_mean) + 0.3 * (pitch_std / 50) + 0.3 * (1 - speech_rate / 5.0)
    }
    
    # Normalize scores
    total = sum(emotion_scores.values())
    if total > 0:
        emotion_scores = {k: v / total for k, v in emotion_scores.items()}
    
    # Get the score for the expected emotion
    match_score = emotion_scores.get(expected_emotion.lower(), 0.0)
    
    # Get the highest confidence
    top_confidence = max(emotion_scores.values())
    
    return {
        "emotion_match_score": match_score,
        "emotion_confidence": top_confidence,
        "predicted_emotions": [{"label": emotion, "score": score} 
                              for emotion, score in sorted(emotion_scores.items(), 
                                                          key=lambda x: x[1], reverse=True)[:3]]
    }

def evaluate_prosody(audio_path: str) -> Dict[str, float]:
    """
    Evaluate the prosody (intonation, rhythm, stress) of the speech.
    
    Args:
        audio_path: Path to the audio file
        
    Returns:
        Dictionary with prosody metrics
    """
    audio, sr = load_audio(audio_path)
    features = extract_audio_features(audio, sr)
    
    # Calculate prosody variability metrics
    # Higher variability generally indicates more expressive speech
    
    # Pitch variability (fundamental frequency)
    valid_f0 = features['f0'][~np.isnan(features['f0'])]
    if len(valid_f0) > 0:
        pitch_variability = np.std(valid_f0) / (np.mean(valid_f0) + 1e-5)
    else:
        pitch_variability = 0.0
    
    # Energy/volume variability
    energy_variability = np.std(features['rms']) / (np.mean(features['rms']) + 1e-5)
    
    # Spectral variability
    spectral_variability = np.std(features['spectral_centroid']) / (np.mean(features['spectral_centroid']) + 1e-5)
    
    # Combine metrics into an overall prosody score
    # Scale values to be between 0 and 1
    pitch_score = min(1.0, pitch_variability / 0.5)  # 0.5 is a heuristic "good" value
    energy_score = min(1.0, energy_variability / 0.5)
    spectral_score = min(1.0, spectral_variability / 0.5)
    
    # Weight and combine scores
    prosody_score = 0.4 * pitch_score + 0.4 * energy_score + 0.2 * spectral_score
    
    return {
        "pitch_variability": float(pitch_variability),
        "energy_variability": float(energy_variability),
        "spectral_variability": float(spectral_variability),
        "prosody_score": float(prosody_score)
    }

def evaluate_expressivity(audio_path: str, expected_emotion: str) -> ExpressivityMetrics:
    """
    Evaluate the expressivity of the speech, combining emotion recognition and prosody.
    
    Args:
        audio_path: Path to the audio file
        expected_emotion: Expected emotion label
        
    Returns:
        ExpressivityMetrics object with various expressivity scores
    """
    # Evaluate emotion recognition
    emotion_metrics = evaluate_emotion_recognition(audio_path, expected_emotion)
    
    # Evaluate prosody
    prosody_metrics = evaluate_prosody(audio_path)
    
    # Combine metrics into an overall expressivity score
    # Weight emotion match and prosody equally
    overall_expressivity = 0.5 * emotion_metrics["emotion_match_score"] + 0.5 * prosody_metrics["prosody_score"]
    
    return ExpressivityMetrics(
        emotion_match_score=emotion_metrics["emotion_match_score"],
        emotion_confidence=emotion_metrics["emotion_confidence"],
        prosody_score=prosody_metrics["prosody_score"],
        overall_expressivity=overall_expressivity
    )

def evaluate_pronunciation(audio_path: str, transcript: str) -> PronunciationMetrics:
    """
    Evaluate the pronunciation quality of the speech.
    
    Args:
        audio_path: Path to the audio file
        transcript: Expected transcript text
        
    Returns:
        PronunciationMetrics object with various pronunciation scores
    """
    # This is a placeholder that would normally use an ASR system
    # to evaluate pronunciation accuracy
    
    # If speechmetrics is available, use it for clarity evaluation
    clarity_score = 0.8  # Default placeholder value
    if SPEECHMETRICS_AVAILABLE:
        try:
            metrics = speechmetrics.load('clarity')
            clarity_score = metrics(audio_path)['clarity']
        except Exception as e:
            print(f"Error computing clarity score: {e}")
    
    # Calculate speaking rate (words per second)
    audio, sr = load_audio(audio_path)
    duration = len(audio) / sr
    word_count = len(transcript.split())
    speaking_rate = word_count / duration if duration > 0 else 0
    
    # Ideal word error rate would come from an ASR system
    # For now, use a placeholder value
    word_error_rate = 0.1  # Default placeholder value
    
    # Combined score
    overall_pronunciation = (0.5 * clarity_score + 0.5 * (1.0 - word_error_rate))
    
    return PronunciationMetrics(
        clarity_score=clarity_score,
        word_error_rate=word_error_rate,
        speaking_rate=speaking_rate,
        overall_pronunciation=overall_pronunciation
    )

def predict_mos(audio_path: str) -> float:
    """
    Predict Mean Opinion Score (MOS) for the audio.
    MOS is a subjective quality metric ranging from 1 (bad) to 5 (excellent).
    
    Args:
        audio_path: Path to the audio file
        
    Returns:
        Predicted MOS score (1.0-5.0)
    """
    # This is a placeholder for a MOS prediction model
    # In a full implementation, this would use a trained model
    
    # Simple heuristic based on audio features for demonstration
    audio, sr = load_audio(audio_path)
    features = extract_audio_features(audio, sr)
    
    # Signal-to-noise ratio approximation
    signal_power = np.mean(features['rms'] ** 2)
    noise_estimate = np.percentile(features['rms'] ** 2, 10)  # Use bottom 10% as noise estimate
    snr = 10 * np.log10(signal_power / (noise_estimate + 1e-10))
    snr_score = min(1.0, max(0.0, snr / 20.0))  # Normalize to 0-1 (20dB is good)
    
    # Spectral balance
    spec_balance = 1.0 - np.std(np.mean(librosa.feature.melspectrogram(y=audio, sr=sr), axis=1)) / 5.0
    spec_balance = max(0.0, min(1.0, spec_balance))
    
    # Combine into MOS prediction (scale from 1 to 5)
    mos = 1.0 + 4.0 * (0.7 * snr_score + 0.3 * spec_balance)
    
    return float(mos)

def evaluate_sample(audio_path: str, transcript: str, expected_emotion: str) -> OverallMetrics:
    """
    Perform comprehensive evaluation of a speech sample.
    
    Args:
        audio_path: Path to the audio file
        transcript: Expected transcript text
        expected_emotion: Expected emotion label
        
    Returns:
        OverallMetrics object with all evaluation metrics
    """
    expressivity_metrics = evaluate_expressivity(audio_path, expected_emotion)
    pronunciation_metrics = evaluate_pronunciation(audio_path, transcript)
    mos_prediction = predict_mos(audio_path)
    
    return OverallMetrics(
        expressivity=expressivity_metrics,
        pronunciation=pronunciation_metrics,
        mos_prediction=mos_prediction
    )

def batch_evaluate(sample_list: List[Dict[str, str]]) -> Dict[str, Dict[str, float]]:
    """
    Evaluate a batch of samples and compute average metrics.
    
    Args:
        sample_list: List of dictionaries, each containing:
                    - 'audio_path': Path to audio file
                    - 'transcript': Expected transcript
                    - 'emotion': Expected emotion
                    
    Returns:
        Dictionary of overall and average metrics
    """
    results = {}
    expressivity_scores = []
    pronunciation_scores = []
    mos_scores = []
    
    for i, sample in enumerate(sample_list):
        audio_path = sample['audio_path']
        transcript = sample['transcript']
        emotion = sample['emotion']
        
        try:
            metrics = evaluate_sample(audio_path, transcript, emotion)
            
            # Store individual result
            results[f"sample_{i}"] = {
                "expressivity": {
                    "emotion_match": metrics.expressivity.emotion_match_score,
                    "prosody": metrics.expressivity.prosody_score,
                    "overall": metrics.expressivity.overall_expressivity
                },
                "pronunciation": {
                    "clarity": metrics.pronunciation.clarity_score,
                    "word_error_rate": metrics.pronunciation.word_error_rate,
                    "overall": metrics.pronunciation.overall_pronunciation
                },
                "mos": metrics.mos_prediction
            }
            
            # Collect for averages
            expressivity_scores.append(metrics.expressivity.overall_expressivity)
            pronunciation_scores.append(metrics.pronunciation.overall_pronunciation)
            mos_scores.append(metrics.mos_prediction)
            
        except Exception as e:
            print(f"Error evaluating sample {i} ({audio_path}): {e}")
            results[f"sample_{i}"] = {"error": str(e)}
    
    # Calculate averages
    if expressivity_scores:
        results["average"] = {
            "expressivity": float(np.mean(expressivity_scores)),
            "pronunciation": float(np.mean(pronunciation_scores)),
            "mos": float(np.mean(mos_scores)),
            "sample_count": len(expressivity_scores)
        }
    
    return results

def save_evaluation_results(results: Dict, output_path: str):
    """Save evaluation results to a JSON file."""
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"Evaluation results saved to {output_path}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Evaluate speech samples for expressivity and quality")
    parser.add_argument("--audio", type=str, required=True, help="Path to audio file or directory")
    parser.add_argument("--transcript", type=str, help="Expected transcript text")
    parser.add_argument("--emotion", type=str, default="neutral", help="Expected emotion")
    parser.add_argument("--output", type=str, help="Output file for evaluation results")
    parser.add_argument("--batch", action="store_true", help="Process multiple files in a directory")
    
    args = parser.parse_args()
    
    if args.batch:
        # Process all audio files in directory
        if not os.path.isdir(args.audio):
            print(f"Error: {args.audio} is not a directory")
            exit(1)
            
        samples = []
        for filename in os.listdir(args.audio):
            if filename.endswith(('.wav', '.mp3', '.flac')):
                audio_path = os.path.join(args.audio, filename)
                # Try to find corresponding text file
                base_name = os.path.splitext(filename)[0]
                transcript_path = os.path.join(args.audio, f"{base_name}.txt")
                json_path = os.path.join(args.audio, f"{base_name}.json")
                
                transcript = args.transcript  # Default to CLI argument
                emotion = args.emotion  # Default to CLI argument
                
                # Try to load transcript and metadata from text or JSON file
                if os.path.exists(transcript_path):
                    with open(transcript_path, 'r') as f:
                        transcript = f.read().strip()
                        
                if os.path.exists(json_path):
                    with open(json_path, 'r') as f:
                        metadata = json.load(f)
                        if 'text' in metadata:
                            transcript = metadata['text']
                        if 'emotion' in metadata:
                            emotion = metadata['emotion']
                
                if transcript:
                    samples.append({
                        'audio_path': audio_path,
                        'transcript': transcript,
                        'emotion': emotion
                    })
        
        if samples:
            results = batch_evaluate(samples)
            if args.output:
                save_evaluation_results(results, args.output)
            else:
                print(json.dumps(results, indent=2))
        else:
            print("No valid samples found for evaluation")
            
    else:
        # Process single file
        if not os.path.isfile(args.audio):
            print(f"Error: {args.audio} does not exist")
            exit(1)
            
        if not args.transcript:
            # Try to find transcript in corresponding text or JSON file
            base_name = os.path.splitext(args.audio)[0]
            transcript_path = f"{base_name}.txt"
            json_path = f"{base_name}.json"
            
            if os.path.exists(transcript_path):
                with open(transcript_path, 'r') as f:
                    args.transcript = f.read().strip()
            elif os.path.exists(json_path):
                with open(json_path, 'r') as f:
                    metadata = json.load(f)
                    if 'text' in metadata:
                        args.transcript = metadata['text']
                    if 'emotion' in metadata and not args.emotion:
                        args.emotion = metadata['emotion']
        
        if not args.transcript:
            print("Error: No transcript provided")
            exit(1)
            
        metrics = evaluate_sample(args.audio, args.transcript, args.emotion)
        
        results = {
            "expressivity": {
                "emotion_match": metrics.expressivity.emotion_match_score,
                "emotion_confidence": metrics.expressivity.emotion_confidence,
                "prosody": metrics.expressivity.prosody_score,
                "overall": metrics.expressivity.overall_expressivity
            },
            "pronunciation": {
                "clarity": metrics.pronunciation.clarity_score,
                "word_error_rate": metrics.pronunciation.word_error_rate,
                "speaking_rate": metrics.pronunciation.speaking_rate,
                "overall": metrics.pronunciation.overall_pronunciation
            },
            "mos": metrics.mos_prediction
        }
        
        if args.output:
            save_evaluation_results(results, args.output)
        else:
            print(json.dumps(results, indent=2)) 